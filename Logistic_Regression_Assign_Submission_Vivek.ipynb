{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic_Regression_Assign_Submission_Vivek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-Regularized Logistic Regression Accuracy: 1.0000\n",
      "\n",
      "Feature Importance (L1 Regularization):\n",
      "             Feature  Coefficient\n",
      "0  sepal length (cm)     0.000000\n",
      "1   sepal width (cm)     2.364839\n",
      "2  petal length (cm)    -2.674099\n",
      "3   petal width (cm)     0.000000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model with L1 regularization (Lasso)\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"L1-Regularized Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"\\nFeature Importance (L1 Regularization):\")\n",
    "print(pd.DataFrame({'Feature': iris.feature_names, 'Coefficient': clf.coef_[0]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2-Regularized Logistic Regression Accuracy: 1.0000\n",
      "\n",
      "Feature Importance (L2 Regularization):\n",
      "             Feature  Coefficient\n",
      "0  sepal length (cm)     0.371123\n",
      "1   sepal width (cm)     1.409712\n",
      "2  petal length (cm)    -2.152101\n",
      "3   petal width (cm)    -0.954742\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model with L2 regularization (Ridge)\n",
    "clf = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display coefficients\n",
    "print(\"\\nFeature Importance (L2 Regularization):\")\n",
    "print(pd.DataFrame({'Feature': iris.feature_names, 'Coefficient': clf.coef_[0]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regularized Logistic Regression Accuracy: 1.0000\n",
      "\n",
      "Feature Importance (Elastic Net Regularization):\n",
      "             Feature  Coefficient\n",
      "0  sepal length (cm)     0.388836\n",
      "1   sepal width (cm)     1.774436\n",
      "2  petal length (cm)    -2.423860\n",
      "3   petal width (cm)    -0.704301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model with Elastic Net regularization\n",
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Elastic Net Regularized Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "print(\"\\nFeature Importance (Elastic Net Regularization):\")\n",
    "print(pd.DataFrame({'Feature': iris.feature_names, 'Coefficient': clf.coef_[0]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Logistic Regression (OvR) Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model using One-vs-Rest (OvR)\n",
    "clf = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Multiclass Logistic Regression (OvR) Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
      "Optimized Logistic Regression Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for C and penalty\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2']  # L1 (Lasso) and L2 (Ridge)\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)  # 'liblinear' supports L1/L2\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "clf_best = LogisticRegression(**best_params, solver='liblinear', max_iter=200, random_state=42)\n",
    "clf_best.fit(X_train, y_train)\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Logistic Regression Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for each fold: [1.0, 0.9666666666666667, 0.9333333333333333, 1.0, 0.9333333333333333]\n",
      "Average Accuracy across folds: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Define Stratified K-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "\n",
    "# Store accuracies from each fold\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Compute average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracies for each fold: {accuracies}\")\n",
    "print(f\"Average Accuracy across folds: {average_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.8045\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/Vivek Bharti/Desktop/titanic.csv\")\n",
    "\n",
    "# Drop irrelevant columns (Name, Ticket, Cabin)\n",
    "df = df.drop(columns=['Name', 'Ticket', 'Cabin'])  # These don't contribute much to survival prediction\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)  # Fill missing ages with median\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)  # Fill missing embarked values with mode\n",
    "\n",
    "# Convert categorical columns using One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Survived'])  # Features\n",
    "y = df['Survived']  # Target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.46415888336127775}\n",
      "Optimized Logistic Regression Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Range of regularization strengths\n",
    "    'penalty': ['l1', 'l2'],  # Lasso (L1) and Ridge (L2)\n",
    "    'solver': ['liblinear', 'saga']  # Compatible solvers for L1/L2\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "random_search = RandomizedSearchCV(clf, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "clf_best = LogisticRegression(**best_params, max_iter=500, random_state=42)\n",
    "\n",
    "clf_best.fit(X_train, y_train)\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Logistic Regression Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Logistic Regression (OvO) Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model using One-vs-One (OvO)\n",
    "clf = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Multiclass Logistic Regression (OvO) Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  # Binary classification (0 = malignant, 1 = benign)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Malignant\", \"Benign\"], yticklabels=[\"Malignant\", \"Benign\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Class Weight Balancing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       356\n",
      "           1       0.82      0.73      0.77        44\n",
      "\n",
      "    accuracy                           0.95       400\n",
      "   macro avg       0.89      0.85      0.87       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "With Balanced Class Weights:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96       356\n",
      "           1       0.61      0.95      0.74        44\n",
      "\n",
      "    accuracy                           0.93       400\n",
      "   macro avg       0.80      0.94      0.85       400\n",
      "weighted avg       0.95      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=10, weights=[0.90, 0.10], random_state=42)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression without class weights\n",
    "clf_default = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_default.fit(X_train, y_train)\n",
    "y_pred_default = clf_default.predict(X_test)\n",
    "\n",
    "# Train Logistic Regression with balanced class weights\n",
    "clf_balanced = LogisticRegression(class_weight='balanced', max_iter=200, random_state=42)\n",
    "clf_balanced.fit(X_train, y_train)\n",
    "y_pred_balanced = clf_balanced.predict(X_test)\n",
    "\n",
    "# Print classification report for both models\n",
    "print(\"\\nWithout Class Weight Balancing:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "print(\"\\nWith Balanced Class Weights:\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy on Titanic Data: 0.8045\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = pd.read_csv(\"C:/Users/Vivek Bharti/Desktop/titanic.csv\")\n",
    "\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['Name', 'Ticket', 'Cabin'])  # These don't contribute much to survival prediction\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)  # Fill missing ages with median age\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)  # Fill missing embarked values with mode\n",
    "\n",
    "# Convert categorical columns using One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Survived'])  # Features\n",
    "y = df['Survived']  # Target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Logistic Regression Model Accuracy on Titanic Data: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 1.0000\n",
      "Accuracy with scaling: 1.0000\n",
      "Feature scaling had no significant impact on Logistic Regression accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression without scaling\n",
    "clf_unscaled = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "# Apply feature scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression with scaled data\n",
    "clf_scaled = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
    "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
    "\n",
    "# Compare accuracy\n",
    "if accuracy_scaled > accuracy_unscaled:\n",
    "    print(\"Feature scaling improved accuracy!\")\n",
    "elif accuracy_scaled < accuracy_unscaled:\n",
    "    print(\"Feature scaling reduced accuracy, but it may help other models.\")\n",
    "else:\n",
    "    print(\"Feature scaling had no significant impact on Logistic Regression accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Convert to binary classification (class 0 vs. other classes)\n",
    "y_binary = (y == 0).astype(int)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for test set\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print results\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy (C=0.5): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model with C=0.5\n",
    "clf = LogisticRegression(C=0.5, max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Logistic Regression Model Accuracy (C=0.5): {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores:\n",
      "             Feature  Coefficient\n",
      "1   sepal width (cm)     0.960718\n",
      "0  sepal length (cm)    -0.397184\n",
      "3   petal width (cm)    -1.003087\n",
      "2  petal length (cm)    -2.373917\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance (model coefficients)\n",
    "feature_importance = clf.coef_[0]  # Coefficients for first class\n",
    "\n",
    "# Display feature importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': feature_importance})\n",
    "importance_df = importance_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Scores:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen’s Kappa Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute Cohen's Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Cohen’s Kappa Score: {kappa_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  # Binary classification (0 = malignant, 1 = benign)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for test set\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Compute Precision-Recall values\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Compute AUC of Precision-Recall Curve\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})', color=\"blue\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve for Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: liblinear - Accuracy: 1.0000\n",
      "Solver: saga - Accuracy: 1.0000\n",
      "Solver: lbfgs - Accuracy: 1.0000\n",
      "\n",
      "Best solver based on accuracy: liblinear (1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define solvers to compare\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "accuracy_results = {}\n",
    "\n",
    "# Train and evaluate models for each solver\n",
    "for solver in solvers:\n",
    "    clf = LogisticRegression(solver=solver, max_iter=200, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_results[solver] = accuracy\n",
    "    print(f\"Solver: {solver} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare accuracy\n",
    "best_solver = max(accuracy_results, key=accuracy_results.get)\n",
    "print(f\"\\nBest solver based on accuracy: {best_solver} ({accuracy_results[best_solver]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  # Binary classification (0 = malignant, 1 = benign)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute MCC score\n",
    "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 1.0000\n",
      "Accuracy with scaling: 1.0000\n",
      "Feature scaling had no significant impact on Logistic Regression accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression on raw data\n",
    "clf_raw = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_raw.fit(X_train, y_train)\n",
    "y_pred_raw = clf_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on standardized data\n",
    "clf_scaled = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Accuracy without scaling: {accuracy_raw:.4f}\")\n",
    "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
    "\n",
    "# Compare accuracy results\n",
    "if accuracy_scaled > accuracy_raw:\n",
    "    print(\"Feature scaling improved model accuracy!\")\n",
    "elif accuracy_scaled < accuracy_raw:\n",
    "    print(\"Feature scaling reduced accuracy, but it might benefit other models.\")\n",
    "else:\n",
    "    print(\"Feature scaling had no significant impact on Logistic Regression accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Regularization Strength (C): 215.44346900318823\n",
      "Optimized Logistic Regression Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for regularization strength C\n",
    "param_grid = {'C': np.logspace(-3, 3, 10)}  # Values from 0.001 to 1000\n",
    "\n",
    "# Apply GridSearchCV for hyperparameter tuning\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best C value\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Optimal Regularization Strength (C): {best_C}\")\n",
    "\n",
    "# Train Logistic Regression with best C\n",
    "clf_best = LogisticRegression(C=best_C, solver='liblinear', max_iter=200, random_state=42)\n",
    "clf_best.fit(X_train, y_train)\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Logistic Regression Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n",
      "Model loaded successfully!\n",
      "Accuracy using the loaded model: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save trained model using joblib\n",
    "joblib.dump(clf, \"logistic_regression_model.pkl\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Load the saved model\n",
    "clf_loaded = joblib.load(\"logistic_regression_model.pkl\")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Predict on the test set using the loaded model\n",
    "y_pred = clf_loaded.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using the loaded model: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --- The_End---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
